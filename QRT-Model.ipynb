{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8V9YOy-GaBe"
      },
      "source": [
        "# 1 Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "doYO1se7HA52"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import spearmanr\n",
        "import xgboost as xgb\n",
        "\n",
        "# Filter out warnings\n",
        "os.environ['PYTHONWARNINGS']='ignore'\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jnA2yWyG1zZ"
      },
      "source": [
        "# 2 Loading the Data & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocessing Functions\n",
        "\n",
        "- `trim_tail` function: trim the tail of the data to reduce the influence of extreme values.\n",
        "- `do_knn_impute` function: perform KNN imputation of missing values.\n",
        "- `load_preprocess` function: apply the entire preprocessing trasformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5L8G_BEgGoyf"
      },
      "outputs": [],
      "source": [
        "def trim_tail(data, lower_percentile, upper_percentile):\n",
        "    \"\"\"\n",
        "    Trim the tail of the data to reduce the influence of extreme values.\n",
        "\n",
        "    Args:\n",
        "    - data (array_like): The data to be trimmed.\n",
        "    - lower_percentile (float): Lower percentile for trimming.\n",
        "    - upper_percentile (float): Upper percentile for trimming.\n",
        "\n",
        "    Returns:\n",
        "    - array_like: Trimmed data.\n",
        "    \"\"\"\n",
        "    low, high = np.percentile(data, [lower_percentile, upper_percentile])\n",
        "    return np.clip(data, low, high)\n",
        "\n",
        "def do_knn_impute(train_data, test_data, neighbors):\n",
        "    \"\"\"\n",
        "    Perform KNN imputation on train and test data.\n",
        "\n",
        "    Args:\n",
        "    - train_data (DataFrame): Training data to be imputed.\n",
        "    - test_data (DataFrame): Test data to be imputed.\n",
        "    - neighbors (int): Number of neighbors for KNN imputation.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Tuple containing imputed train and test data.\n",
        "    \"\"\"\n",
        "    knn_imputer = KNNImputer(n_neighbors=neighbors)\n",
        "    train_data_filled = knn_imputer.fit_transform(train_data)\n",
        "    test_data_filled = knn_imputer.transform(test_data)\n",
        "\n",
        "    columns_lst = list(train_data.columns)\n",
        "    train_data = pd.DataFrame(train_data_filled, columns=columns_lst, index=train_data.index)\n",
        "    test_data = pd.DataFrame(test_data_filled, columns=columns_lst, index=test_data.index)\n",
        "    return train_data, test_data\n",
        "\n",
        "def load_preprocess(separate_country=False):\n",
        "    \"\"\"\n",
        "    Load and preprocess the data.\n",
        "\n",
        "    Args:\n",
        "    - separate_country (bool): Whether to separate data by country.\n",
        "\n",
        "    Returns:\n",
        "    - list or tuple: Preprocessed data depending on the value of `separate_country`.\n",
        "    \"\"\"\n",
        "    # Load train and set\n",
        "    X_train = pd.read_csv('X_train.csv', index_col='ID').drop(columns=[\"DAY_ID\"])\n",
        "    Y_train = pd.read_csv('y_train.csv', index_col='ID')\n",
        "    X_test = pd.read_csv('X_test.csv', index_col='ID').drop(columns=[\"DAY_ID\"])\n",
        "    Y_test = pd.read_csv('y_test_random_final.csv', index_col='ID')\n",
        "\n",
        "    if separate_country:\n",
        "        # Join features and target for preprocessing\n",
        "        train_df = X_train.join(Y_train)\n",
        "        test_df = X_test.join(Y_test)\n",
        "\n",
        "        # Split training data into DE and FR datasets | trim tail\n",
        "        train_fr = train_df[train_df.COUNTRY=='FR'].sort_index().drop(columns=[\"COUNTRY\"])\n",
        "        train_de = train_df[train_df.COUNTRY=='DE'].sort_index().drop(columns=[\"COUNTRY\"])\n",
        "\n",
        "        # Split test data into DE and FR datasets | trim tail\n",
        "        test_fr = test_df[test_df.COUNTRY=='FR'].sort_index().drop(columns=[\"COUNTRY\"])\n",
        "        test_de = test_df[test_df.COUNTRY=='DE'].sort_index().drop(columns=[\"COUNTRY\"])\n",
        "\n",
        "        # KNN imputer\n",
        "        print(\"KNN imputer started ...\")\n",
        "        train_fr, test_fr = do_knn_impute(train_fr, test_fr, 5)\n",
        "        train_de, test_de = do_knn_impute(train_de, test_de, 5)\n",
        "        print(\"KNN imputer end ! \")\n",
        "\n",
        "        # Trim tail\n",
        "        trim_list = ['DE_CONSUMPTION', 'FR_CONSUMPTION',\n",
        "                     'DE_FR_EXCHANGE', 'FR_DE_EXCHANGE', 'DE_NET_EXPORT', 'FR_NET_EXPORT',\n",
        "                     'DE_NET_IMPORT', 'FR_NET_IMPORT','DE_RESIDUAL_LOAD', 'FR_RESIDUAL_LOAD']\n",
        "        print(\"Trim tail started, on columns: {}\".format(trim_list))\n",
        "\n",
        "        trimmed_train_fr = trim_tail(train_fr[trim_list], 1, 99)\n",
        "        trimmed_train_de = trim_tail(train_de[trim_list], 1, 99)\n",
        "        trimmed_test_fr = trim_tail(test_fr[trim_list], 1, 99)\n",
        "        trimmed_test_de = trim_tail(test_de[trim_list], 1, 99)\n",
        "\n",
        "        train_fr[trim_list] = trimmed_train_fr\n",
        "        train_de[trim_list] = trimmed_train_de\n",
        "        test_fr[trim_list] = trimmed_test_fr\n",
        "        test_de[trim_list] = trimmed_test_de\n",
        "\n",
        "        X_train_fr = train_fr.drop(columns=['TARGET'])\n",
        "        X_train_de = train_de.drop(columns=['TARGET'])\n",
        "        Y_train_fr = train_fr[['TARGET']]\n",
        "        Y_train_de = train_de[['TARGET']]\n",
        "\n",
        "        X_test_fr = test_fr.drop(columns=['TARGET'])\n",
        "        X_test_de = test_de.drop(columns=['TARGET'])\n",
        "        Y_test_fr = test_fr[['TARGET']]\n",
        "        Y_test_de = test_de[['TARGET']]\n",
        "        print(\"Preprocessing finished !\")\n",
        "\n",
        "        return [X_train_fr, Y_train_fr, X_train_de, Y_train_de, X_test_fr, Y_test_fr, X_test_de, Y_test_de]\n",
        "    else:\n",
        "        ohc = OneHotEncoder(drop='first')\n",
        "        X_train['COUNTRY'] = ohc.fit_transform(X_train.COUNTRY.values.reshape(-1, 1)).toarray()\n",
        "        X_test['COUNTRY'] = ohc.fit_transform(X_test.COUNTRY.values.reshape(-1, 1)).toarray()\n",
        "\n",
        "        return X_train, Y_train, X_test, Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying preprocessing\n",
        "\n",
        "#### Creating separate DE and FR preprocessed datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRywwkaXVfOd",
        "outputId": "55442da3-272f-44a6-81aa-95828d2b264b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN imputer started ...\n",
            "KNN imputer end ! \n",
            "Trim tail started, on columns: ['DE_CONSUMPTION', 'FR_CONSUMPTION', 'DE_FR_EXCHANGE', 'FR_DE_EXCHANGE', 'DE_NET_EXPORT', 'FR_NET_EXPORT', 'DE_NET_IMPORT', 'FR_NET_IMPORT', 'DE_RESIDUAL_LOAD', 'FR_RESIDUAL_LOAD']\n",
            "Preprocessing finished !\n"
          ]
        }
      ],
      "source": [
        "# Apply preprocessing\n",
        "X_train_fr, Y_train_fr, X_train_de, Y_train_de, X_test_fr, Y_test_fr, X_test_de, Y_test_de = load_preprocess(separate_country=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating full preprocessed dataset containing both DE and FR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gcYY9A60F7qr"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing\n",
        "X_train, Y_train, X_test, Y_test = load_preprocess(separate_country=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A416DiMZbLGa"
      },
      "source": [
        "## 3.1 Lag items:  built in-week lag items for DE & FR seperately"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lagged features might have an impact on future values of our target variable, therefore, including them could potentially increase model performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e0CnrKWJVbj1"
      },
      "outputs": [],
      "source": [
        "def add_lagged_features(df):\n",
        "    \"\"\"\n",
        "    Add lagged features to the DataFrame.\n",
        "\n",
        "    Args:\n",
        "    - df (DataFrame): The DataFrame to which lagged features will be added.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: The DataFrame with lagged features added.\n",
        "    \"\"\"\n",
        "    cols = ['DE_CONSUMPTION', 'FR_CONSUMPTION', 'DE_NET_EXPORT', 'FR_NET_EXPORT',\n",
        "            'DE_RESIDUAL_LOAD', 'FR_RESIDUAL_LOAD', 'GAS_RET', 'COAL_RET', 'CARBON_RET']\n",
        "\n",
        "    lagged_features = pd.DataFrame(index=df.index)\n",
        "    for col in cols:\n",
        "        for i in range(1, 5):\n",
        "            lagged_col_name = f'{col}_LAG{i}'\n",
        "            lagged_features[lagged_col_name] = df[col].shift(i)\n",
        "\n",
        "    df = pd.concat([df, lagged_features], axis=1)\n",
        "    df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bqUM-eqbWfQ"
      },
      "source": [
        "## 3.2 Consumption Inspirations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `consumption_insp` function:\n",
        "\n",
        "1. **Average Commodity Price Variations:**\n",
        "   - `avg_c_p_v`: Calculates the average commodity price variations by summing the values of 'GAS_RET', 'COAL_RET', and 'CARBON_RET', and then taking the mean.\n",
        "   - `avg_c_p_v_ma5`: Computes a rolling mean of `avg_c_p_v` over a window of 5 time periods.\n",
        "   - `avg_c_p_v_ma10`: Computes a rolling mean of `avg_c_p_v` over a window of 10 time periods.\n",
        "\n",
        "2. **Nuclear Ratio Trend:**\n",
        "   - `nuclear_ratio_fr`: Computes the ratio of 'FR_NUCLEAR' to the total sum of energy sources including gas, coal, hydro, nuclear, solar, and wind power for France.\n",
        "   - `nuclear_ratio_de`: Computes the ratio of 'DE_NUCLEAR' to the total sum of energy sources including gas, coal, hydro, nuclear, solar, and wind power for Germany.\n",
        "\n",
        "3. **New Energy Transform Efficiency:**\n",
        "   - `hydro_rain_fr`: Calculates the efficiency of hydro energy transformation for France by dividing 'FR_HYDRO' by the rolling mean of 'FR_RAIN' over a window of 5 time periods.\n",
        "   - `hydro_rain_de`: Calculates the efficiency of hydro energy transformation for Germany by dividing 'DE_HYDRO' by the rolling mean of 'DE_RAIN' over a window of 5 time periods.\n",
        "   - `wind_windpow_fr`: Calculates the efficiency of wind power transformation for France by dividing 'FR_WINDPOW' by the rolling mean of 'FR_WIND' over a window of 2 time periods.\n",
        "   - `wind_windpow_de`: Calculates the efficiency of wind power transformation for Germany by dividing 'DE_WINDPOW' by the rolling mean of 'DE_WIND' over a window of 2 time periods.\n",
        "\n",
        "4. **Residual Load Premium Cost:**\n",
        "   - `load_premium_cost_fr`: Estimates the premium cost of residual load for France by multiplying 'FR_RESIDUAL_LOAD' with the average commodity price variation.\n",
        "   - `load_premium_cost_de`: Estimates the premium cost of residual load for Germany by multiplying 'DE_RESIDUAL_LOAD' with the average commodity price variation.\n",
        "   - `import_cost_fr`: Estimates the import cost for France by multiplying 'FR_NET_IMPORT' with the average commodity price variation.\n",
        "   - `import_cost_de`: Estimates the import cost for Germany by multiplying 'DE_NET_IMPORT' with the average commodity price variation.\n",
        "\n",
        "5. **Standardization of New Features:**\n",
        "   - The newly generated features are standardized using `StandardScaler` to ensure that each feature contributes equally to the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vb4qSWJpCYMj"
      },
      "outputs": [],
      "source": [
        "def consumption_insp(df):\n",
        "    \"\"\"\n",
        "    Perform consumption inspection and feature engineering on the DataFrame.\n",
        "\n",
        "    Args:\n",
        "    - df (DataFrame): The DataFrame to perform inspection and feature engineering on.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: The DataFrame with consumption inspection and engineered features.\n",
        "    \"\"\"\n",
        "    # average commodity price variations\n",
        "    df['avg_c_p_v'] = df[['GAS_RET', 'COAL_RET', 'CARBON_RET']].sum(axis=1).mean()\n",
        "    df['avg_c_p_v_ma5'] = df['avg_c_p_v'].rolling(window=5).mean()\n",
        "    df['avg_c_p_v_ma10'] = df['avg_c_p_v'].rolling(window=10).mean()\n",
        "\n",
        "    # nuclear ratio trend\n",
        "    epsilon = 1e-8\n",
        "    df['nuclear_ratio_fr'] = df['FR_NUCLEAR'] / (df[['FR_GAS', 'FR_COAL', 'FR_HYDRO', 'FR_NUCLEAR', 'FR_SOLAR', 'FR_WINDPOW']].sum(axis=1) + epsilon)\n",
        "    df['nuclear_ratio_de'] = df['DE_NUCLEAR'] / (df[['DE_GAS', 'DE_COAL', 'DE_HYDRO', 'DE_NUCLEAR', 'DE_SOLAR', 'DE_WINDPOW']].sum(axis=1) + epsilon)\n",
        "\n",
        "    # new energy transform efficiency\n",
        "    df['hydro_rain_fr'] = df['FR_HYDRO'] / (df['FR_RAIN'].rolling(window=5).mean() + epsilon)\n",
        "    df['hydro_rain_de'] = df['DE_HYDRO'] / (df['DE_RAIN'].rolling(window=5).mean() + epsilon)\n",
        "\n",
        "    df['wind_windpow_fr'] = df['FR_WINDPOW'] / (df['FR_WIND'].rolling(window=2).mean() + epsilon)\n",
        "    df['wind_windpow_de'] = df['DE_WINDPOW'] / (df['DE_WIND'].rolling(window=2).mean() + epsilon)\n",
        "\n",
        "    # residual_load premium cost\n",
        "    df['load_premium_cost_fr'] = df['FR_RESIDUAL_LOAD'] * (df['avg_c_p_v'] + epsilon)\n",
        "    df['load_premium_cost_de'] = df['DE_RESIDUAL_LOAD'] * (df['avg_c_p_v'] + epsilon)\n",
        "    df['import_cost_fr'] = df['FR_NET_IMPORT'] * (df['avg_c_p_v'] + epsilon)\n",
        "    df['import_cost_de'] = df['DE_NET_IMPORT'] * (df['avg_c_p_v'] + epsilon)\n",
        "\n",
        "    # standardize the new features\n",
        "    features_to_scale = ['hydro_rain_fr', 'hydro_rain_de', 'wind_windpow_fr', 'wind_windpow_de',\n",
        "                         'load_premium_cost_fr', 'load_premium_cost_de', 'import_cost_fr', 'import_cost_de']\n",
        "    \n",
        "    # initialize scaler\n",
        "    scaler = StandardScaler()\n",
        "    df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
        "    df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k8jzEpJeiVF"
      },
      "source": [
        "# Seperated DE & FR data for Next Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding features to the FR dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OnK1FL7ICMxq"
      },
      "outputs": [],
      "source": [
        "# Add lagged features to training and testing data\n",
        "X_train_fr = add_lagged_features(X_train_fr)\n",
        "X_test_fr = add_lagged_features(X_test_fr)\n",
        "\n",
        "# Perform consumption inspection on training and testing data\n",
        "X_train_fr = consumption_insp(X_train_fr)\n",
        "X_test_fr = consumption_insp(X_test_fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding features to the DE dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "83SM1WWoeuxT"
      },
      "outputs": [],
      "source": [
        "# Add lagged features to training and testing data\n",
        "X_train_de = add_lagged_features(X_train_de)\n",
        "X_test_de = add_lagged_features(X_test_de)\n",
        "\n",
        "# Perform consumption inspection on training and testing data\n",
        "X_train_de = consumption_insp(X_train_de)\n",
        "X_test_de = consumption_insp(X_test_de)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding features to the full dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2jL-NSurpjDY"
      },
      "outputs": [],
      "source": [
        "# Add lagged features to training and testing data\n",
        "X_train = add_lagged_features(X_train)\n",
        "X_test = add_lagged_features(X_test)\n",
        "\n",
        "# Perform consumption inspection on training and testing data\n",
        "X_train = consumption_insp(X_train)\n",
        "X_test = consumption_insp(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL5Sme2eo1LS"
      },
      "source": [
        "# Modeling ToolKit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functions for the training and evaluation of models:\n",
        "\n",
        "- `metric_kit` function computes and displays evaluation metrics for regression: Mean Squared Error (MSE) and Mean Absolute Error (MAE). Computes Spearman correlation coefficient.\n",
        "- `modeling_pipeline` function trains a regression model using a pipeline: Splits data into training and validation sets (80:20 ratio). Performs hyperparameter tuning using grid search with cross-validation. Selects the best model based on minimizing mean squared error. Prints the best parameters found during hyperparameter tuning. Evaluates the model using `metric_kit` function. Returns the best model selected after hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4GvsBgZuGvXg"
      },
      "outputs": [],
      "source": [
        "def metric_kit(y_test, y_pred):\n",
        "    \"\"\"\n",
        "    Compute and display evaluation metrics for regression.\n",
        "\n",
        "    Parameters:\n",
        "    y_test (array-like): True target values.\n",
        "    y_pred (array-like): Predicted target values.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Compute Mean Squared Error (MSE)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\" MSE: {mse}\")\n",
        "\n",
        "    # Compute Mean Absolute Error (MAE)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print(f\" MAE: {mae}\")\n",
        "\n",
        "    # Compute Spearman correlation\n",
        "    spear = spearmanr(y_test, y_pred).correlation\n",
        "    print(' Spearman correlation: {:.1f}%'.format(100 * spear))\n",
        "\n",
        "def modeling_pipeline(X_train, Y_train, model_regressor, param_grid):\n",
        "    \"\"\"\n",
        "    Train a regression model using a pipeline consisting of data splitting, hyperparameter tuning,\n",
        "    and model fitting.\n",
        "\n",
        "    Parameters:\n",
        "    X_train (array-like): Features for training.\n",
        "    Y_train (array-like): Target values for training.\n",
        "    model_regressor (estimator): Regression model object.\n",
        "    param_grid (dict): Parameter grid for hyperparameter tuning.\n",
        "\n",
        "    Returns:\n",
        "    estimator: Best model selected after hyperparameter tuning.\n",
        "    \"\"\"\n",
        "    # Split data into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=2024)\n",
        "\n",
        "    # Define the model\n",
        "    model = model_regressor\n",
        "\n",
        "    # Hyperparameter tuning\n",
        "    grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best parameters\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "    # Use the best model to make predictions\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred_best = best_model.predict(X_val)\n",
        "\n",
        "    # Show the evaluation metrics\n",
        "    metric_kit(y_val, y_pred_best)\n",
        "\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG2qFffxGp1e"
      },
      "source": [
        "# 4 Decision Tree Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BILRZVAhJ4-",
        "outputId": "0bd438ef-fb08-4aea-ae53-92dd8a3245f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
            " MSE: 1.1148123404153867\n",
            " MAE: 0.5510857896134241\n",
            " Spearman correlation: 21.7%\n",
            "Best parameters: {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
            " MSE: 0.9808938504781785\n",
            " MAE: 0.6362261728323381\n",
            " Spearman correlation: 43.5%\n"
          ]
        }
      ],
      "source": [
        "# Define model\n",
        "model_regressor = DecisionTreeRegressor(random_state=2024)\n",
        "\n",
        "# Define parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'criterion': ['squared_error', 'friedman_mse', 'absolute_error'],  # 对于较新版本的scikit-learn\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Train model for dataset X_train_fr\n",
        "model_fr = modeling_pipeline(X_train_fr, Y_train_fr, model_regressor, param_grid)\n",
        "\n",
        "# Define model\n",
        "model_regressor = DecisionTreeRegressor(random_state=2024)\n",
        "\n",
        "# Train model for dataset X_train_de\n",
        "model_de = modeling_pipeline(X_train_de, Y_train_de, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5P87uXHqOTN",
        "outputId": "75f5c057-8a7b-4232-8096-2ecab4a7a28a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'criterion': 'absolute_error', 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
            " MSE: 1.4075318814209123\n",
            " MAE: 0.6865750418754671\n",
            " Spearman correlation: -1.8%\n"
          ]
        }
      ],
      "source": [
        "# Define parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'criterion': ['squared_error', 'friedman_mse', 'absolute_error'],  # 对于较新版本的scikit-learn\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Define model\n",
        "model_regressor = DecisionTreeRegressor(random_state=2024)\n",
        "\n",
        "# Train model for full X_train\n",
        "model = modeling_pipeline(X_train, Y_train, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXKPupR3Gv-e"
      },
      "source": [
        "# 5 Random Forest Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyObWxq0Gytl",
        "outputId": "cc8d3bd0-e2eb-4269-f6e1-796c894440bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'max_depth': 15, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
            " MSE: 0.987878190720214\n",
            " MAE: 0.518452050289056\n",
            " Spearman correlation: 7.0%\n",
            "Best parameters: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
            " MSE: 0.5354197519041137\n",
            " MAE: 0.47802824672598826\n",
            " Spearman correlation: 57.4%\n"
          ]
        }
      ],
      "source": [
        "model_regressor = RandomForestRegressor(random_state=2024)\n",
        "param_grid = {\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [None, 15, 25],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 4]\n",
        "}\n",
        "model_fr = modeling_pipeline(X_train_fr, Y_train_fr, model_regressor, param_grid)\n",
        "\n",
        "model_regressor = RandomForestRegressor(random_state=2024)\n",
        "model_de = modeling_pipeline(X_train_de, Y_train_de, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKl1gDaPrehO",
        "outputId": "2f57ff11-00ca-4030-bb27-83fa0fede33f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'max_depth': 15, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
            " MSE: 1.1493312839664984\n",
            " MAE: 0.6328470432773299\n",
            " Spearman correlation: 10.5%\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'n_estimators':         [100],\n",
        "    'max_depth':            [None, 15, 25],\n",
        "    'min_samples_split':    [2, 5],\n",
        "    'min_samples_leaf':     [1, 4]\n",
        "}\n",
        "model_regressor = RandomForestRegressor(random_state=2024)\n",
        "model = modeling_pipeline(X_train, Y_train, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8CyZcHEsHms"
      },
      "source": [
        "# 6 Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIFfGmYXsq3d",
        "outputId": "e1eab44c-aa76-4ac1-cea9-64bbf7d454ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 200}\n",
            " MSE: 0.9870948515012374\n",
            " MAE: 0.5095648525197348\n",
            " Spearman correlation: 13.1%\n",
            "Best parameters: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100}\n",
            " MSE: 0.5560701342404257\n",
            " MAE: 0.49512243617098145\n",
            " Spearman correlation: 54.4%\n"
          ]
        }
      ],
      "source": [
        "model_regressor = BaggingRegressor(random_state=2024)\n",
        "param_grid = {\n",
        "    'n_estimators'      : [100, 200],\n",
        "    'max_samples'       : [0.5, 0.7, 1.0],\n",
        "    'max_features'      : [0.5, 0.7, 1.0],\n",
        "    'bootstrap'         : [True],\n",
        "    'bootstrap_features': [False]\n",
        "}\n",
        "model_fr = modeling_pipeline(X_train_fr, Y_train_fr, model_regressor, param_grid)\n",
        "\n",
        "model_regressor = BaggingRegressor(random_state=2024)\n",
        "model_de = modeling_pipeline(X_train_de, Y_train_de, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWJVWfUsu2AE",
        "outputId": "a0135748-85bd-45ae-fdfd-80fc3ede81d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 0.5, 'n_estimators': 200}\n",
            " MSE: 1.1335105417195022\n",
            " MAE: 0.6245125328606613\n",
            " Spearman correlation: 14.1%\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'n_estimators'      : [100, 200],\n",
        "    'max_samples'       : [0.5, 0.7, 1.0],\n",
        "    'max_features'      : [0.5, 0.7, 1.0],\n",
        "    'bootstrap'         : [True],\n",
        "    'bootstrap_features': [False]\n",
        "}\n",
        "model_regressor = BaggingRegressor(random_state=2024)\n",
        "model = modeling_pipeline(X_train, Y_train, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXaeL9UnxBf_"
      },
      "source": [
        "# 7 AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyxOzbCXxH7y",
        "outputId": "4f406422-9535-4e3b-eb1b-801a76ba136a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'learning_rate': 0.1, 'n_estimators': 100}\n",
            " MSE: 0.9996764966311997\n",
            " MAE: 0.48710182886718906\n",
            " Spearman correlation: 1.0%\n",
            "Best parameters: {'learning_rate': 0.1, 'n_estimators': 100}\n",
            " MSE: 0.6264037921372445\n",
            " MAE: 0.545109722388572\n",
            " Spearman correlation: 55.0%\n"
          ]
        }
      ],
      "source": [
        "model_regressor = AdaBoostRegressor(random_state=2024)\n",
        "param_grid = {\n",
        "    'n_estimators':  [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "}\n",
        "model_fr = modeling_pipeline(X_train_fr, Y_train_fr, model_regressor, param_grid)\n",
        "\n",
        "model_regressor = AdaBoostRegressor(random_state=2024)\n",
        "model_de = modeling_pipeline(X_train_de, Y_train_de, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REBUgwGoxMPj",
        "outputId": "75e20598-99be-473b-e875-f143079a89a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'learning_rate': 0.01, 'n_estimators': 50}\n",
            " MSE: 1.104825813959231\n",
            " MAE: 0.6077906679172808\n",
            " Spearman correlation: 11.0%\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'n_estimators':  [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "}\n",
        "model_regressor = AdaBoostRegressor(random_state=2024)\n",
        "model = modeling_pipeline(X_train, Y_train, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGQsJrDax4TK"
      },
      "source": [
        "# 8 Gradient Boosting (scikit-learn implementation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyFESx4Sx_5U",
        "outputId": "6c51e43b-576e-446a-adca-3901b77099f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'learning_rate': 0.01, 'n_estimators': 50}\n",
            " MSE: 0.9702509063251323\n",
            " MAE: 0.4610495130440358\n",
            " Spearman correlation: -2.3%\n",
            "Best parameters: {'learning_rate': 0.05, 'n_estimators': 50}\n",
            " MSE: 0.5815684914213218\n",
            " MAE: 0.5014346516325487\n",
            " Spearman correlation: 55.3%\n"
          ]
        }
      ],
      "source": [
        "model_regressor = GradientBoostingRegressor(random_state=2024)\n",
        "param_grid = {\n",
        "    'n_estimators':  [50, 100, 150, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "}\n",
        "model_fr = modeling_pipeline(X_train_fr, Y_train_fr, model_regressor, param_grid)\n",
        "\n",
        "model_regressor = GradientBoostingRegressor(random_state=2024)\n",
        "model_de = modeling_pipeline(X_train_de, Y_train_de, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDBM7wqcyC3w",
        "outputId": "ff95f1aa-c2e7-4016-d74c-3fdba19b6e52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'learning_rate': 0.01, 'n_estimators': 50}\n",
            " MSE: 1.102957901463703\n",
            " MAE: 0.5974752622768578\n",
            " Spearman correlation: 17.2%\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'n_estimators':  [50, 100, 150, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "}\n",
        "model_regressor = GradientBoostingRegressor(random_state=2024)\n",
        "model = modeling_pipeline(X_train, Y_train, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz3KA2-hyN_-"
      },
      "source": [
        "# 9 Extra Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsfqt7AhyQ7U",
        "outputId": "db9a3706-de1c-457e-df8a-2186affacfc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'max_depth': 20, 'n_estimators': 100}\n",
            " MSE: 1.0299490327632566\n",
            " MAE: 0.5581992411538986\n",
            " Spearman correlation: 6.4%\n",
            "Best parameters: {'max_depth': None, 'n_estimators': 500}\n",
            " MSE: 0.5415273246472665\n",
            " MAE: 0.4749256019357339\n",
            " Spearman correlation: 59.5%\n"
          ]
        }
      ],
      "source": [
        "model_regressor = ExtraTreesRegressor(random_state=2024)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 300, 500],\n",
        "    'max_depth':    [None, 10, 20],\n",
        "}\n",
        "model_fr = modeling_pipeline(X_train_fr, Y_train_fr, model_regressor, param_grid)\n",
        "\n",
        "model_regressor = ExtraTreesRegressor(random_state=2024)\n",
        "model_de = modeling_pipeline(X_train_de, Y_train_de, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrsLzrH4ySwZ",
        "outputId": "dc21421c-5bee-4376-e297-d3d77ac3589f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'max_depth': 10, 'n_estimators': 500}\n",
            " MSE: 1.093453106619794\n",
            " MAE: 0.6041149929140268\n",
            " Spearman correlation: 26.8%\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150, 300, 500],\n",
        "    'max_depth':    [None, 10, 20],\n",
        "}\n",
        "model_regressor = ExtraTreesRegressor(random_state=2024)\n",
        "model = modeling_pipeline(X_train, Y_train, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ8byuS3ybsv"
      },
      "source": [
        "# 10 XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPlhSnztyeRM",
        "outputId": "17e9dc6a-e94d-4d47-967a-88f54d592ca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
            " MSE: 0.974274317321525\n",
            " MAE: 0.4646354567317139\n",
            " Spearman correlation: -5.8%\n",
            "Best parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 150}\n",
            " MSE: 0.5374254449517155\n",
            " MAE: 0.474040847024263\n",
            " Spearman correlation: 58.4%\n"
          ]
        }
      ],
      "source": [
        "model_regressor = xgb.XGBRegressor(random_state=2024)\n",
        "param_grid = {\n",
        "    'n_estimators':  [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "    'max_depth':     [3, 5, 7],\n",
        "}\n",
        "model_fr = modeling_pipeline(X_train_fr, Y_train_fr, model_regressor, param_grid)\n",
        "\n",
        "model_regressor = xgb.XGBRegressor(random_state=2024)\n",
        "model_de = modeling_pipeline(X_train_de, Y_train_de, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjuqGWs4ygQj",
        "outputId": "2f998784-f900-429e-d85a-de87679e3bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
            " MSE: 1.1023341942163798\n",
            " MAE: 0.5958034559405495\n",
            " Spearman correlation: 10.1%\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'n_estimators':  [50, 100, 150],\n",
        "    'learning_rate': [0.01, 0.1, 1.0],\n",
        "    'max_depth':     [3, 5, 7],\n",
        "}\n",
        "model_regressor = xgb.XGBRegressor(random_state=2024)\n",
        "model = modeling_pipeline(X_train, Y_train, model_regressor, param_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4UM48pSpKd8"
      },
      "source": [
        "# Submission ToolKit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GB4t1R2ipCAy"
      },
      "outputs": [],
      "source": [
        "# Y_test_fr[:] = model.predict(X_test_fr)\n",
        "\n",
        "# Y_test_fr.to_csv('benchmark_qrt_fr.csv', header = True, index = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6jnA2yWyG1zZ",
        "A416DiMZbLGa",
        "_bqUM-eqbWfQ",
        "PWU64xvHesJg",
        "N0okOcHtetpV",
        "kL5Sme2eo1LS",
        "zG2qFffxGp1e",
        "zXKPupR3Gv-e",
        "F4UM48pSpKd8"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
