{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6jnA2yWyG1zZ",
        "A416DiMZbLGa",
        "_bqUM-eqbWfQ",
        "PWU64xvHesJg",
        "N0okOcHtetpV",
        "kL5Sme2eo1LS",
        "zG2qFffxGp1e",
        "zXKPupR3Gv-e",
        "F4UM48pSpKd8"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Library"
      ],
      "metadata": {
        "id": "G8V9YOy-GaBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer"
      ],
      "metadata": {
        "id": "doYO1se7HA52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Loading the Data & Preprocessing"
      ],
      "metadata": {
        "id": "6jnA2yWyG1zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trim tail: to reduce the influence of extreme values\n",
        "def trim_tail(data, lower_percentile, upper_percentile):\n",
        "  low, high = np.percentile(data, [lower_percentile, upper_percentile])\n",
        "  return np.clip(data, low, high)\n",
        "\n",
        "def do_knn_impute(train_data, test_data, neighbors):\n",
        "  # knn impute\n",
        "  knn_imputer     = KNNImputer(n_neighbors=neighbors)\n",
        "  train_data_filled  = knn_imputer.fit_transform(train_data)\n",
        "  test_data_filled   = knn_imputer.transform(test_data)\n",
        "\n",
        "  # transform to df\n",
        "  columns_lst       = list(train_data.columns)\n",
        "  train_data     = pd.DataFrame(train_data_filled, columns = columns_lst, index=train_data.index)\n",
        "  test_data      = pd.DataFrame(test_data_filled, columns = columns_lst, index=test_data.index)\n",
        "  return train_data, test_data\n",
        "\n",
        "# load_preprocess:\n",
        "#  1. load the data\n",
        "#  2. Split the DE & FR\n",
        "#  3. KNN imputer\n",
        "#  4. trim tail on certain features\n",
        "def load_preprocess(separate_country=False):\n",
        "  # 1. Load train and set\n",
        "  X_train     = pd.read_csv('X_train.csv', index_col='ID').drop(columns=[\"DAY_ID\"])\n",
        "  Y_train     = pd.read_csv('y_train.csv', index_col='ID')\n",
        "  X_test      = pd.read_csv('X_test.csv', index_col='ID').drop(columns=[\"DAY_ID\"])\n",
        "  Y_test      = pd.read_csv('y_test_random_final.csv', index_col='ID')\n",
        "\n",
        "  if separate_country:\n",
        "      #   Join features and target for preprocessing\n",
        "      train_df    = X_train.join(Y_train)\n",
        "      test_df     = X_test.join(Y_test)\n",
        "\n",
        "      # 2. Split training data into DE and FR datasets | trim tail\n",
        "      train_fr    = train_df[train_df.COUNTRY=='FR'].sort_index().drop(columns=[\"COUNTRY\"])\n",
        "      train_de    = train_df[train_df.COUNTRY=='DE'].sort_index().drop(columns=[\"COUNTRY\"])\n",
        "\n",
        "      #   Split test data into DE and FR datasets | trim tail\n",
        "      test_fr     = test_df[test_df.COUNTRY=='FR'].sort_index().drop(columns=[\"COUNTRY\"])\n",
        "      test_de     = test_df[test_df.COUNTRY=='DE'].sort_index().drop(columns=[\"COUNTRY\"])\n",
        "\n",
        "      # 3. KNN imputer\n",
        "      print(\"KNN imputer started ...\")\n",
        "      train_fr, test_fr = do_knn_impute(train_fr, test_fr, 5)\n",
        "      train_de, test_de = do_knn_impute(train_de, test_de, 5)\n",
        "      print(\"KNN imputer end ! \")\n",
        "\n",
        "      # 4. trim tail\n",
        "      #   Choose the features that contain outliers\n",
        "      trim_list = ['DE_CONSUMPTION', 'FR_CONSUMPTION',\n",
        "      'DE_FR_EXCHANGE', 'FR_DE_EXCHANGE', 'DE_NET_EXPORT', 'FR_NET_EXPORT',\n",
        "      'DE_NET_IMPORT', 'FR_NET_IMPORT','DE_RESIDUAL_LOAD', 'FR_RESIDUAL_LOAD']\n",
        "      print(\"Trim tail started, on columns: {}\".format(trim_list))\n",
        "\n",
        "      #   Trim the train data for FR & DE\n",
        "      trimmed_train_fr = trim_tail(train_fr[trim_list], 1, 99)\n",
        "      trimmed_train_de = trim_tail(train_de[trim_list], 1, 99)\n",
        "      trimmed_test_fr  = trim_tail(test_fr[trim_list], 1, 99)\n",
        "      trimmed_test_de  = trim_tail(test_de[trim_list], 1, 99)\n",
        "\n",
        "      #   replace original features with trimed features\n",
        "      train_fr[trim_list] = trimmed_train_fr\n",
        "      train_de[trim_list] = trimmed_train_de\n",
        "      test_fr[trim_list] = trimmed_test_fr\n",
        "      test_de[trim_list] = trimmed_test_de\n",
        "\n",
        "      #   seperate the features and target\n",
        "      X_train_fr  = train_fr.drop(columns=['TARGET'])\n",
        "      X_train_de  = train_de.drop(columns=['TARGET'])\n",
        "      Y_train_fr  = train_fr[['TARGET']]\n",
        "      Y_train_de  = train_de[['TARGET']]\n",
        "\n",
        "      X_test_fr  = test_fr.drop(columns=['TARGET'])\n",
        "      X_test_de  = test_de.drop(columns=['TARGET'])\n",
        "      Y_test_fr  = test_fr[['TARGET']]\n",
        "      Y_test_de  = test_de[['TARGET']]\n",
        "      print(\"Proprocessing finished !\")\n",
        "\n",
        "      return [X_train_fr, Y_train_fr, X_train_de, Y_train_de, X_test_fr, Y_test_fr, X_test_de, Y_test_de]\n",
        "\n",
        "  # If NOT separate country then return full train and test data\n",
        "  else:\n",
        "      ohc         = OneHotEncoder(drop='first')\n",
        "      X_train['COUNTRY']  = ohc.fit_transform(X_train.COUNTRY.values.reshape(-1,1)).toarray()\n",
        "      X_train.drop(columns=['DAY_ID'], inplace=True)\n",
        "\n",
        "      X_test['COUNTRY']   = ohc.fit_transform(X_test.COUNTRY.values.reshape(-1,1)).toarray()\n",
        "      X_test.drop(columns=['DAY_ID'], inplace=True)\n",
        "\n",
        "      return X_train, Y_train, X_test, Y_test"
      ],
      "metadata": {
        "id": "5L8G_BEgGoyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_fr, Y_train_fr, X_train_de, Y_train_de, X_test_fr, Y_test_fr, X_test_de, Y_test_de = load_preprocess(separate_country=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRywwkaXVfOd",
        "outputId": "b643d747-02ee-46bd-9f51-e0be1c5e24c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN imputer started ...\n",
            "KNN imputer end ! \n",
            "Trim tail started, on columns: ['DE_CONSUMPTION', 'FR_CONSUMPTION', 'DE_FR_EXCHANGE', 'FR_DE_EXCHANGE', 'DE_NET_EXPORT', 'FR_NET_EXPORT', 'DE_NET_IMPORT', 'FR_NET_IMPORT', 'DE_RESIDUAL_LOAD', 'FR_RESIDUAL_LOAD']\n",
            "Proprocessing finished !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Featur Engineering"
      ],
      "metadata": {
        "id": "y2Kk6TFBGmSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Lag items:  built in-week lag items for DE & FR seperately"
      ],
      "metadata": {
        "id": "A416DiMZbLGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.errors import PerformanceWarning\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore', category=PerformanceWarning)\n",
        "\n",
        "def add_lagged_features(df):\n",
        "    cols = ['DE_CONSUMPTION', 'FR_CONSUMPTION', 'DE_NET_EXPORT', 'FR_NET_EXPORT',\n",
        "    'DE_RESIDUAL_LOAD', 'FR_RESIDUAL_LOAD', 'GAS_RET', 'COAL_RET', 'CARBON_RET']\n",
        "\n",
        "    lagged_features = pd.DataFrame(index=df.index)\n",
        "    for col in cols:\n",
        "        for i in range(1, 3):\n",
        "            lagged_col_name = f'{col}_LAG{i}'\n",
        "            lagged_features[lagged_col_name] = df[col].shift(i)\n",
        "\n",
        "    df = pd.concat([df, lagged_features], axis=1)\n",
        "    df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "e0CnrKWJVbj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Consumption Inspirations"
      ],
      "metadata": {
        "id": "_bqUM-eqbWfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def consumption_insp(df):\n",
        "  # average commodity price variations\n",
        "  df['avg_c_p_v']    = df[['GAS_RET', 'COAL_RET',\t'CARBON_RET']].sum(axis=1).mean()\n",
        "  df['avg_c_p_v_ma5']  = df['avg_c_p_v'].rolling(window=5).mean()\n",
        "  df['avg_c_p_v_ma10']  = df['avg_c_p_v'].rolling(window=10).mean()\n",
        "\n",
        "  # nuclear ratio trend\n",
        "  epsilon = 1e-8\n",
        "  df['nuclear_ratio_fr'] = df['FR_NUCLEAR'] / (df[['FR_GAS', 'FR_COAL', 'FR_HYDRO', 'FR_NUCLEAR', 'FR_SOLAR', 'FR_WINDPOW']].sum(axis=1) + epsilon)\n",
        "  df['nuclear_ratio_de'] = df['DE_NUCLEAR'] / (df[['DE_GAS', 'DE_COAL', 'DE_HYDRO', 'DE_NUCLEAR', 'DE_SOLAR', 'DE_WINDPOW']].sum(axis=1) + epsilon)\n",
        "\n",
        "  # new energy transform efficency\n",
        "  df['hydro_rain_fr'] = df['FR_HYDRO'] / (df['FR_RAIN'].rolling(window=5).mean() + epsilon)\n",
        "  df['hydro_rain_de'] = df['DE_HYDRO'] / (df['DE_RAIN'].rolling(window=5).mean() + epsilon)\n",
        "\n",
        "  df['wind_windpow_fr'] = df['FR_WINDPOW'] / (df['FR_WIND'].rolling(window=2).mean() + epsilon)\n",
        "  df['wind_windpow_de'] = df['DE_WINDPOW'] / (df['DE_WIND'].rolling(window=2).mean() + epsilon)\n",
        "\n",
        "  # residual_load premium cost\n",
        "  df['load_premium_cost_fr']  = df['FR_RESIDUAL_LOAD'] * (df['avg_c_p_v'] + epsilon)\n",
        "  df['load_premium_cost_de']  = df['DE_RESIDUAL_LOAD'] * (df['avg_c_p_v'] + epsilon)\n",
        "  df['import_cost_fr']  = df['FR_NET_IMPORT'] * (df['avg_c_p_v'] + epsilon)\n",
        "  df['import_cost_de']  = df['DE_NET_IMPORT'] * (df['avg_c_p_v'] + epsilon)\n",
        "\n",
        "  # standerdize the new features\n",
        "  features_to_scale = ['hydro_rain_fr', 'hydro_rain_de', 'wind_windpow_fr', 'wind_windpow_de',\n",
        "             'load_premium_cost_fr', 'load_premium_cost_de', 'import_cost_fr', 'import_cost_de']\n",
        "  # initialize scaler\n",
        "  scaler = StandardScaler()\n",
        "  df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
        "  df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "vb4qSWJpCYMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Data Set for Next Modeling\n"
      ],
      "metadata": {
        "id": "_k8jzEpJeiVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FR"
      ],
      "metadata": {
        "id": "PWU64xvHesJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_fr = add_lagged_features(X_train_fr)\n",
        "X_test_fr = add_lagged_features(X_test_fr)\n",
        "X_train_fr = consumption_insp(X_train_fr)\n",
        "X_test_fr = consumption_insp(X_test_fr)\n",
        "\n",
        "Y_train_fr\n",
        "Y_test_fr"
      ],
      "metadata": {
        "id": "OnK1FL7ICMxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DE"
      ],
      "metadata": {
        "id": "N0okOcHtetpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_de = add_lagged_features(X_train_de)\n",
        "X_test_de = add_lagged_features(X_test_de)\n",
        "X_train_de = consumption_insp(X_train_de)\n",
        "X_test_de = consumption_insp(X_test_de)\n",
        "\n",
        "Y_train_de\n",
        "Y_test_de"
      ],
      "metadata": {
        "id": "83SM1WWoeuxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling ToolKit"
      ],
      "metadata": {
        "id": "kL5Sme2eo1LS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def metric_kit(y_test, y_pred):\n",
        "  # show the MSE\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "  print(f\"[ Default Decision Tree ] MSE: {mse}\")\n",
        "\n",
        "  # show the spearmanr\n",
        "  spear = spearmanr(y_test, y_pred).correlation\n",
        "  print('Spearman correlation - default: {:.1f}%'.format(100 * spear))\n",
        "\n",
        "def modeling_pipeline(X_train, Y_train, model_regressor, param_grid):\n",
        "  # train valid set split\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train_fr, Y_train_fr, test_size=0.2, random_state=2024)\n",
        "\n",
        "  # DT model\n",
        "  model = model_regressor\n",
        "\n",
        "  # train model\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # test model\n",
        "  y_pred = model.predict(X_val)\n",
        "\n",
        "  # show the metrics\n",
        "  metric_kit(y_val, y_pred)\n",
        "\n",
        "  # hyperparameter tuning\n",
        "  grid_search = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "\n",
        "  # get the best parameters\n",
        "  print(f\"best_params: {grid_search.best_params_}\")\n",
        "\n",
        "  # use best parameter to predict\n",
        "  best_model = grid_search.best_estimator_\n",
        "  y_pred_best = best_model.predict(X_val)\n",
        "\n",
        "  # show the metrics\n",
        "  metric_kit(y_val, y_pred_best)\n",
        "\n",
        "  return best_model"
      ],
      "metadata": {
        "id": "4GvsBgZuGvXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Decision Tree Modeling"
      ],
      "metadata": {
        "id": "zG2qFffxGp1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_regressor = DecisionTreeRegressor(random_state=2024)\n",
        "param_grid    = {\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "            'min_samples_split': [2, 10, 20],\n",
        "            'min_samples_leaf': [1, 5, 10]\n",
        "          }\n",
        "model = modeling_pipeline(X_train_fr, Y_train_fr, model_regressor, param_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BILRZVAhJ4-",
        "outputId": "5cba5d42-0cad-47b2-b982-c0d76db86487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Default Decision Tree ] MSE: 1.999358661781256\n",
            "Spearman correlation - default: -1.9%\n",
            "best_params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
            "[ Default Decision Tree ] MSE: 1.2060422106188151\n",
            "Spearman correlation - default: 0.9%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Random Forest Modeling"
      ],
      "metadata": {
        "id": "zXKPupR3Gv-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_regressor = RandomForestRegressor(n_estimators=100, random_state=2024)\n",
        "param_grid    = {\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "            'min_samples_split': [2, 10, 20],\n",
        "            'min_samples_leaf': [1, 5, 10]\n",
        "          }\n",
        "model = modeling_pipeline(X_train_fr, Y_train_fr, model_regressor, param_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "eyObWxq0Gytl",
        "outputId": "8bf1ce35-4e9b-4e6e-bee0-fa945a8f998f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Default Decision Tree ] MSE: 1.0018365896021084\n",
            "Spearman correlation - default: 21.5%\n",
            "best_params: {'max_depth': 10, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
            "[ Default Decision Tree ] MSE: 0.9674437564374735\n",
            "Spearman correlation - default: 15.9%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(random_state=2024)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(random_state=2024)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(random_state=2024)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission ToolKit"
      ],
      "metadata": {
        "id": "F4UM48pSpKd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test_fr[:] = model.predict(X_test_fr)\n",
        "\n",
        "Y_test_fr.to_csv('benchmark_qrt_fr.csv', header = True, index = True)"
      ],
      "metadata": {
        "id": "GB4t1R2ipCAy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
